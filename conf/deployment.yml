# Custom section is used to store configurations that might be repetative.
# Please read YAML documentation for details on how to use substitutions and anchors.
custom:
  basic-cluster-props: &basic-cluster-props
    spark_version: "10.4.x-cpu-ml-scala2.12"

  basic-static-cluster: &basic-static-cluster
    new_cluster:
      <<: *basic-cluster-props
      num_workers: 1
      node_type_id: "Standard_D3_v2"

environments:
  default:
    workflows:
      #######################################################################################
      #   Example workflow for integration tests                                            #
      #######################################################################################
      - name: "kb_devops_proj-sample-tests"
        tasks:
          - task_key: "main"
            <<: *basic-static-cluster
            spark_python_task:
                python_file: "file://tests/entrypoint.py"
                # this call supports all standard pytest arguments
                parameters: ["file:fuse://tests/integration", "--cov=kb_devops_proj"]
      #######################################################################################
      #   Example workflow for dbt tests                                            #
      #######################################################################################
      - name: "dbt_task_manifest_test"
        tasks:
          - task_key: "dbt_task_manifest_test"
            <<: *basic-static-cluster
            dbt_task:
                parameters: ["file:fuse:conf/tasks/dbt_task_config.json"]
      #######################################################################################
      # this is an example job with single ETL task based on 2.1 API and wheel_task format #
      ######################################################################################